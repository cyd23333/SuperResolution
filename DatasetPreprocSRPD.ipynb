{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "rootDir = os.getcwd()\n",
    "saveDir = f'{rootDir}/results/'\n",
    "dataset_dir = os.path.join(rootDir, 'datasets/dataSRP/')\n",
    "dataset_savedir = dataset_dir\n",
    "\n",
    "with open(os.path.join(dataset_dir, 'dataDict_originProcValueRange.pkl'), 'rb') as f:\n",
    "    dataDict = pickle.load(f)\n",
    "print('Data Loaded!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 SRPD x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 307365.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully divided dataset!\n",
      "      ProcType: 100x10\n",
      "        train_input: (128000, 300)\n",
      "        val_input: (32000, 300)\n",
      "        train_label: (128000, 3000)\n",
      "        val_label: (32000, 3000)\n"
     ]
    }
   ],
   "source": [
    "def DivideDataset(data:np.ndarray, splitType:str, trainRate:int, valRate:int):\n",
    "    \"\"\"Divide the datasets(len,block_size) into two or three sub-sets, and then return the divided datasets.\n",
    "    ## Args\n",
    "        data (Numpy);  \n",
    "        splitType (string): withTest OR withoutTest;  \n",
    "        trainRate (int): e.g. 8 represents the training set accounts for 8 in 10;    \n",
    "        valRate (int).\n",
    "    ## Returns\n",
    "        trainData (Numpy);    \n",
    "        valData (Numpy);    \n",
    "        (*)testData (Numpy);.   \n",
    "    \"\"\"\n",
    "    data_dim = len(data.shape)\n",
    "    if data_dim != 2:\n",
    "        raise ValueError('Wrong data dimension, the input data should be 2D array')\n",
    "    if splitType == 'withoutTest':\n",
    "        idx = data.shape[0] * trainRate // 10\n",
    "        if data_dim == 1:\n",
    "            trainData = data[:idx]\n",
    "            valData = data[idx:]\n",
    "        elif data_dim == 2:\n",
    "            trainData = data[:idx,:]\n",
    "            valData = data[idx:,:]\n",
    "        return trainData, valData\n",
    "    \n",
    "    elif splitType == 'withTest':\n",
    "        print('withTestSet\\n')\n",
    "        pass\n",
    "    else:\n",
    "        raise SyntaxError('Wrong splitType, please input withTest or withoutTest')\n",
    "    \n",
    "def DownsampDivide(data:np.ndarray, ProcType:str):\n",
    "    \"\"\"N-n downsampling, downsample according to the rate, keeping the first data point of each segment\n",
    "    # Args\n",
    "        data (1D np array): The input data. Defaults to np.ndarray.\n",
    "        ProcType (str): Two types: 10x10 and 100x10. Defaults to str.\n",
    "    # Raises\n",
    "        ValueError: The input data dimension is incorrect\n",
    "    # Returns\n",
    "        train_input, val_input, train_label, val_label: Downsampled training and validation sets\n",
    "    \"\"\"\n",
    "    if len(data.shape) != 1:\n",
    "        raise ValueError('The input data dimension is incorrect and should be 1D array')\n",
    "    if ProcType == '100x10':\n",
    "        inputSet = data[::10].reshape(-1, 300)\n",
    "        labelSet = data.reshape(-1, 3000)\n",
    "        \n",
    "    train_input, val_input = DivideDataset(inputSet, splitType='withoutTest', trainRate=8, valRate=2)\n",
    "    train_label, val_label = DivideDataset(labelSet, splitType='withoutTest', trainRate=8, valRate=2)\n",
    "    \n",
    "    return train_input, val_input, train_label, val_label\n",
    "\n",
    "\n",
    "A_train_input = []; A_val_input = []; A_train_label = []; A_val_label = []\n",
    "ProcType = '100x10'\n",
    "\n",
    "for dataIdx in tqdm(dataDict.keys()):\n",
    "    train_input, val_input, train_label, val_label = DownsampDivide(dataDict[dataIdx].squeeze(), ProcType)\n",
    "    A_train_input.append(train_input)  ; A_val_input.append(val_input)\n",
    "    A_train_label.append(train_label)  ; A_val_label.append(val_label)\n",
    "\n",
    "A_train_input = np.vstack(A_train_input)  ;  A_val_input = np.vstack(A_val_input)\n",
    "A_train_label = np.vstack(A_train_label)  ;  A_val_label = np.vstack(A_val_label)\n",
    "np.save(f'{dataset_savedir}train_input_{ProcType}', A_train_input)\n",
    "np.save(f'{dataset_savedir}val_input_{ProcType}', A_val_input)\n",
    "np.save(f'{dataset_savedir}train_label_{ProcType}', A_train_label)\n",
    "np.save(f'{dataset_savedir}val_label_{ProcType}', A_val_label)\n",
    "\n",
    "print(f'''Successfully divided dataset!\n",
    "      ProcType: {ProcType}\n",
    "        train_input: {A_train_input.shape}\n",
    "        val_input: {A_val_input.shape}\n",
    "        train_label: {A_train_label.shape}\n",
    "        val_label: {A_val_label.shape}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SRPD x20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTrain = np.load(f'{dataset_dir}train_label_100x10.npy')\n",
    "labelVal = np.load(f'{dataset_dir}val_label_100x10.npy')\n",
    "\n",
    "inputTrain = np.load(f'{dataset_dir}train_input_100x10.npy')\n",
    "inputVal = np.load(f'{dataset_dir}val_input_100x10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000, 300) (128000, 3000) (32000, 300) (32000, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(inputTrain), np.shape(labelTrain), np.shape(inputVal), np.shape(labelVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000, 150) (32000, 150)\n"
     ]
    }
   ],
   "source": [
    "inTr = inputTrain[:,::2]\n",
    "inVal = inputVal[:,::2]\n",
    "print(np.shape(inTr), np.shape(inVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "np.save(f'{dataset_savedir}train_input_50x20.npy', inTr)\n",
    "np.save(f'{dataset_savedir}val_input_50x20.npy', inVal)\n",
    "np.save(f'{dataset_savedir}train_label_50x20.npy', labelTrain)\n",
    "np.save(f'{dataset_savedir}val_label_50x20.npy', labelVal)\n",
    "print('Data saved successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SupRes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
